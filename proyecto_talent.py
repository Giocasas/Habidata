# -*- coding: utf-8 -*-
"""Proyecto Talent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-4Sd25JrZYrzOPoQCXyyf6szqEYUCrom
"""

# Librerias
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from joblib import dump

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import warnings

# Para preprocesamiento
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Para modelado
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import joblib

# Para evaluación
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score

# Configuración para visualizaciones
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

# Ignorar advertencias
import warnings
warnings.filterwarnings('ignore')

# Conectar Drive
from google.colab import drive
drive.mount('/content/drive')

# Cargar dataset
data = pd.read_csv("/content/drive/MyDrive/CSV/properties_final.csv")

"""#Entendimiento de los datos"""

# Mostrar las primeras filas del Dataframe
print("\nPrimeras filas del Dataframe")
data.head()

# Mostrar cantidad de filas y columnas
print("\nCantidad de filas y columnas:")
data.shape

# Obtener información general del DataFrame
print("\nInformación del DataFrame:")
data.info()

# Descripción estadística de los datos numéricos
print("\nDescripción Estadística:")
data.describe()

# Verificar valores nulos
print("\nValores nulos por columna:")
data.isnull().sum()

# Verificar tipos de datos
print("\nTipos de datos:")
data.dtypes

"""#Visualización Exploratoria"""

# Función para determinar columnas numéricas y categóricas
def identificar_tipos_columnas(dataframe):
    numericas = dataframe.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categoricas = dataframe.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()
    return numericas, categoricas

columnas_numericas, columnas_categoricas = identificar_tipos_columnas(df)

# Distribución de variables numéricas
print("\nDistribución de variables numéricas:")
plt.figure(figsize=(15, 10))
for i, column in enumerate(columnas_numericas[:min(len(columnas_numericas), 9)]):
    plt.subplot(3, 3, i+1)
    sns.histplot(df[column], kde=True)
    plt.title(f'Distribución de {column}')
plt.tight_layout()
plt.show()

# Boxplots para detectar outliers (valores atípicos)
print("\nDetección de outliers en variables numéricas:")
plt.figure(figsize=(15, 10))
for i, column in enumerate(columnas_numericas[:min(len(columnas_numericas), 9)]):
    plt.subplot(3, 3, i+1)
    sns.boxplot(y=df[column])
    plt.title(f'Boxplot de {column}')
plt.tight_layout()
plt.show()

# Matriz de correlación
print("\nMatriz de correlación entre variables numéricas:")
plt.figure(figsize=(12, 10))
correlation_matrix = df[columnas_numericas].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Matriz de Correlación')
plt.show()

"""#Eliminación de outliers de precio"""

Q1 = data['price'].quantile(0.25)
Q3 = data['price'].quantile(0.75)
IQR = Q3 - Q1

# Filtrar datos dentro de 1.5*IQR del rango intercuartílico
data = data[(data['price'] >= Q1 - 1.5 * IQR) & (data['price'] <= Q3 + 1.5 * IQR)]

# Gráfico de dispersión de precios",
plt.figure(figsize=(10, 6))
plt.scatter(range(len(data)), data['price'], alpha=0.5)
plt.title('Dispersión de precios de propiedades')
plt.xlabel('Índice de muestra')
plt.ylabel('Precio')
plt.grid(True)
plt.show()

# Seleccionar características relevantes
features = ['created_on', 'surface_total_final', 'bedrooms_final', 'bathrooms_final', 'l3_final', 'l4_final']
X = data[features]
y = data['price']

# Dividir en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Tamaño del conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"Tamaño del conjunto de prueba: {X_test.shape[0]} muestras")

# Identificar tipos de columnas
numeric_features = ['surface_total_final', 'bedrooms_final', 'bathrooms_final']
categorical_features = ['l3_final', 'l4_final']

# Crear transformadores para diferentes tipos de columnas
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combinar transformadores usando ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Verificar la forma de los datos después del preprocesamiento
X_train_preprocessed = preprocessor.fit_transform(X_train)
print(f"Forma de X_train después del preprocesamiento: {X_train_preprocessed.shape}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    confusion_matrix, roc_curve, roc_auc_score, classification_report
)

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR

models = {
    "Linear Regression": LinearRegression(),
    "Ridge": Ridge(),
    "Lasso": Lasso(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "Support Vector Regressor": SVR()
}

"""#Validación cruzada, comparación y resultados"""

results = {}
for name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('model', model)])
    scores = cross_validate(pipeline, X_train, y_train,
                            cv=5,
                            scoring=('r2', 'neg_mean_absolute_error', 'neg_root_mean_squared_error'),
                            return_train_score=True)
    results[name] = {
        'R2_mean': np.mean(scores['test_r2']),
        'MAE_mean': -np.mean(scores['test_neg_mean_absolute_error']),
        'RMSE_mean': -np.mean(scores['test_neg_root_mean_squared_error'])
    }

cv_results = pd.DataFrame(results).T.sort_values(by='R2_mean', ascending=False)
print(cv_results)

cv_results[['R2_mean', 'MAE_mean', 'RMSE_mean']].plot(kind='bar', figsize=(12,6))
plt.title("Resultados de validación cruzada")
plt.ylabel("Puntaje promedio (mayor es mejor para R2)")
plt.show()

"""#Entrenar modelo final y evaluar en test (Supongamos que el mejor modelo fue RandomForestRegressor:)"""

best_model = Pipeline(steps=[('preprocessor', preprocessor),
                             ('model', RandomForestRegressor(random_state=42))])

best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

print("R2:", r2_score(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("RMSE:", mean_squared_error(y_test, y_pred))

"""#Búsqueda de hiperparámetros con GridSearchCV"""

param_grid = {
    'model__n_estimators': [100, 200, 300],
    'model__max_depth': [10, 20, None],
    'model__min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(
    Pipeline(steps=[('preprocessor', preprocessor),
                    ('model', RandomForestRegressor(random_state=42))]),
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)

grid.fit(X_train, y_train)
print("Mejores hiperparámetros:", grid.best_params_)
print("Mejor score (R2):", grid.best_score_)

"""#Evaluar el modelo optimizado"""

best_rf = grid.best_estimator_
y_pred_test = best_rf.predict(X_test)

print("R2 (test):", r2_score(y_test, y_pred_test))
print("MAE (test):", mean_absolute_error(y_test, y_pred_test))
print("RMSE (test):", mean_squared_error(y_test, y_pred_test))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras.utils import to_categorical

# Preparar datos preprocesados
X_train_prep = preprocessor.fit_transform(X_train)
X_test_prep = preprocessor.transform(X_test)

input_dim = X_train_prep.shape[1]

model_nn = Sequential([
    Dense(64, activation='relu', input_dim=input_dim),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model_nn.fit(X_train_prep, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)

# Evaluación
loss, acc = model_nn.evaluate(X_test_prep, y_test, verbose=0)
print(f"Accuracy Red Neuronal: {acc:.3f}")

# Curva ROC
y_prob_nn = model_nn.predict(X_test_prep).ravel()
fpr, tpr, _ = roc_curve(y_test, y_prob_nn)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f'Red Neuronal (AUC={roc_auc:.2f})')
plt.legend()
plt.plot([0,1],[0,1],'k--')
plt.title("Curva ROC - Red Neuronal")
plt.show()